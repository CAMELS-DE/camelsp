{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rheinlandpfalz\n",
    "\n",
    "Every federal state is represented by its own input directory and is processed into a NUTS level 2 directory containing a sub-folder for each discharge location. These folder names are derived from NUTS and reflect the CAMELS id. The NUTS level 2 code for Rheinland-Pfalz is `DEB`.\n",
    "\n",
    "To pre-process the data, you need to write (at least) two functions. One should extract all metadata and condense it into a single `pandas.DataFrame`. This is used to build the folder structure and derive the ids.\n",
    "The second function has to take an id, as provided by the state authorities, called `provider_id` and return a `pandas.DataFrame` with the transformed data. The dataframe needs the three columns `['date', 'q' | 'w', 'flag']`.\n",
    "\n",
    "For easier and unified output handling, the `camelsp` package contains a context object called `Bundesland`. It takes a number of names and abbreviations to identify the correct federal state and returns an object that holds helper and save functions.\n",
    "\n",
    "The context saves files as needed and can easily be changed to save files with different strategies, ie. fill missing data with NaN, merge data into a single file, create files for each variable or pack everything together into a netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.errors import ParserError\n",
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict\n",
    "from glob import glob\n",
    "from datetime import datetime as dt\n",
    "from dateparser import parse\n",
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "from camelsp import Bundesland"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context can also be instantiated as any regular Python class, ie. to load only the default input data path, that we will user later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexander/Github/camels/camelsp/input_data/RLP_Rheinland-Pfalz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the context also makes the input path available, if camelsp was install locally\n",
    "BASE = Bundesland('Pfalz').input_path\n",
    "BASE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata reader\n",
    "\n",
    "Define the function that extracts / reads and eventually merges all metadata for this federal state. You can develop the function here, without using the Bundesland context and then later use the context to pass extracted metadata. The Context has a function for saving *raw* metadata, that takes a `pandas.DataFrame` and needs you to identify the id column.\n",
    "Here, *raw* refers to provider metadata, that has not yet been transformed into the CAMELS-de Metadata schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nummer</th>\n",
       "      <th>Stationsname</th>\n",
       "      <th>Gewässer</th>\n",
       "      <th>Aeo</th>\n",
       "      <th>RW</th>\n",
       "      <th>HW</th>\n",
       "      <th>km oh. Münd.</th>\n",
       "      <th>PNP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25460158</td>\n",
       "      <td>Nanzdietschweiler</td>\n",
       "      <td>Glan</td>\n",
       "      <td>200.94</td>\n",
       "      <td>2604686.0</td>\n",
       "      <td>5479753.0</td>\n",
       "      <td>58</td>\n",
       "      <td>215.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25460307</td>\n",
       "      <td>Eschenau</td>\n",
       "      <td>Glan</td>\n",
       "      <td>598.31</td>\n",
       "      <td>2607203.0</td>\n",
       "      <td>5496963.0</td>\n",
       "      <td>33</td>\n",
       "      <td>180.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25460409</td>\n",
       "      <td>Odenbach</td>\n",
       "      <td>Glan</td>\n",
       "      <td>1088.17</td>\n",
       "      <td>2619263.0</td>\n",
       "      <td>5507120.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>147.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25460522</td>\n",
       "      <td>Stausee Ohmbach</td>\n",
       "      <td>Ohmbach</td>\n",
       "      <td>34.50</td>\n",
       "      <td>2600311.0</td>\n",
       "      <td>5476977.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>232.367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25460577</td>\n",
       "      <td>Rodenbach 2</td>\n",
       "      <td>Bruchbach</td>\n",
       "      <td>19.44</td>\n",
       "      <td>2620094.0</td>\n",
       "      <td>5483350.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>220.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>26280366</td>\n",
       "      <td>Bitburg Stausee</td>\n",
       "      <td>Prüm, Stausee Bitburg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>316797.7</td>\n",
       "      <td>5542933.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Zerstört</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>27180108</td>\n",
       "      <td>Müsch</td>\n",
       "      <td>Ahr</td>\n",
       "      <td>352.65</td>\n",
       "      <td>2558880.0</td>\n",
       "      <td>5583865.0</td>\n",
       "      <td>63</td>\n",
       "      <td>292.738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nummer       Stationsname               Gewässer      Aeo         RW  \\\n",
       "1    25460158  Nanzdietschweiler                   Glan   200.94  2604686.0   \n",
       "2    25460307           Eschenau                   Glan   598.31  2607203.0   \n",
       "3    25460409           Odenbach                   Glan  1088.17  2619263.0   \n",
       "4    25460522    Stausee Ohmbach                Ohmbach    34.50  2600311.0   \n",
       "5    25460577        Rodenbach 2              Bruchbach    19.44  2620094.0   \n",
       "..        ...                ...                    ...      ...        ...   \n",
       "159       NaN                NaN                    NaN      NaN        NaN   \n",
       "160  26280366    Bitburg Stausee  Prüm, Stausee Bitburg      NaN   316797.7   \n",
       "161       NaN                NaN                    NaN      NaN        NaN   \n",
       "162  Zerstört                NaN                    NaN      NaN        NaN   \n",
       "163  27180108              Müsch                    Ahr   352.65  2558880.0   \n",
       "\n",
       "            HW km oh. Münd.      PNP  \n",
       "1    5479753.0           58  215.499  \n",
       "2    5496963.0           33  180.334  \n",
       "3    5507120.0         14.5  147.750  \n",
       "4    5476977.0          3.6  232.367  \n",
       "5    5483350.0          1.5  220.764  \n",
       "..         ...          ...      ...  \n",
       "159        NaN          NaN      NaN  \n",
       "160  5542933.3          NaN      NaN  \n",
       "161        NaN          NaN      NaN  \n",
       "162        NaN          NaN      NaN  \n",
       "163  5583865.0           63  292.738  \n",
       "\n",
       "[163 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Bundesland('RLP') as bl:\n",
    "    # read metadata\n",
    "    metadata = pd.read_excel(os.path.join(BASE, 'aktive Pegel_Februar 2022_mit Stammdaten.xlsx'), header=2)\n",
    "\n",
    "    # rename 7th column\n",
    "    metadata.rename(columns={'km': 'km oh. Münd.'}, inplace=True)\n",
    "\n",
    "    # drop first row\n",
    "    metadata = metadata.iloc[1:]\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the id column will be Nummer\n",
    "id_column = 'Nummer'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file extract and parse\n",
    "\n",
    "Here, we need to process the filename as the `'Ort'` is contained in the filename. Looks like the metadata header is **always** to line 32, indicating a finished header by `YTYP;`. Verify this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to left-join the data, as each Stationsnummer exists twice. Thus, it is only the combination of Stationsnummer and variable, that makes the data unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>q</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, q, flag]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_file(nr: Union[int, str], variable: str, input_path: str) -> pd.DataFrame:\n",
    "\n",
    "    # build the path to the correct subfolder:\n",
    "    path = os.path.join(input_path, variable, f\"{nr}_{variable.upper()}.txt\")\n",
    "\n",
    "    # check file\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['date', variable.lower(), 'flag'])\n",
    "    \n",
    "    # read\n",
    "    raw = pd.read_csv(path, skiprows=4, encoding='latin1', sep=' ', header=None)\n",
    "    return pd.DataFrame({\n",
    "        'date': [dt.strptime(str(_)[:8], '%Y%m%d') for _ in raw.iloc[:, 0]],\n",
    "        variable.lower(): raw.iloc[:, 1],\n",
    "        'flag': np.nan\n",
    "    })\n",
    "\n",
    "    return raw\n",
    "\n",
    "extract_file(42, 'q', BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nuts_id provider_id                              path\n",
      "0  DEB10000    25460158  ./DEB/DEB10000/DEB10000_data.csv\n",
      "1  DEB10010    25460307  ./DEB/DEB10010/DEB10010_data.csv\n",
      "2  DEB10020    25460409  ./DEB/DEB10020/DEB10020_data.csv\n",
      "3  DEB10030    25460522  ./DEB/DEB10030/DEB10030_data.csv\n",
      "4  DEB10040    25460577  ./DEB/DEB10040/DEB10040_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:02<00:00, 66.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were warnings during the processing. The log can be found at: /home/alexander/Github/camels/camelsp/output_data/metadata/DEB_error.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Bundesland('RLP') as bl:\n",
    "    # save the metadata\n",
    "    bl.save_raw_metadata(metadata, id_column, overwrite=True)\n",
    "\n",
    "    # for reference, call the nuts-mapping as table\n",
    "    nuts_map = bl.nuts_table\n",
    "    print(nuts_map.head())\n",
    "\n",
    "    \n",
    "    with warnings.catch_warnings(record=True) as warns:\n",
    "        for provider_id in tqdm(metadata[id_column].values.astype(str)):\n",
    "            # get q\n",
    "            q = extract_file(provider_id, 'q', bl.input_path)\n",
    "            w = extract_file(provider_id, 'w', bl.input_path)\n",
    "\n",
    "            bl.save_timeseries(q, provider_id)\n",
    "            bl.save_timeseries(w, provider_id)\n",
    "\n",
    "        # check if there were warnings (there are warnings)\n",
    "        if len(warns) > 0:\n",
    "            log_path = bl.save_warnings(warns)\n",
    "            print(f\"There were warnings during the processing. The log can be found at: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
