{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayern\n",
    "\n",
    "Every federal state is represented by its own input directory and is processed into a NUTS level 2 directory containing a sub-folder for each discharge location. These folder names are derived from NUTS and reflect the CAMELS id. The NUTS level 2 code for Bayern is `DE2`.\n",
    "\n",
    "To pre-process the data, you need to write (at least) two functions. One should extract all metadata and condense it into a single `pandas.DataFrame`. This is used to build the folder structure and derive the ids.\n",
    "The second function has to take an id, as provided by the state authorities, called `provider_id` and return a `pandas.DataFrame` with the transformed data. The dataframe needs the three columns `['date', 'q' | 'w', 'flag']`.\n",
    "\n",
    "For easier and unified output handling, the `camelsp` package contains a context object called `Bundesland`. It takes a number of names and abbreviations to identify the correct federal state and returns an object that holds helper and save functions.\n",
    "\n",
    "The context saves files as needed and can easily be changed to save files with different strategies, ie. fill missing data with NaN, merge data into a single file, create files for each variable or pack everything together into a netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict\n",
    "import zipfile\n",
    "from datetime import datetime as dt\n",
    "from io import StringIO\n",
    "import warnings\n",
    "\n",
    "from camelsp import Bundesland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context can also be instantiated as any regular Python class, ie. to load only the default input data path, that we will user later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the context also makes the input path available, if camelsp was install locally\n",
    "BASE = Bundesland('Bayern').input_path\n",
    "BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata reader\n",
    "\n",
    "Define the function that extracts / reads and eventually merges all metadata for this federal state. You can develop the function here, without using the Bundesland context and then later use the context to pass extracted metadata. The Context has a function for saving *raw* metadata, that takes a `pandas.DataFrame` and needs you to identify the id column.\n",
    "Here, *raw* refers to provider metadata, that has not yet been transformed into the CAMELS-de Metadata schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function \n",
    "def read_meta(base_path) -> pd.DataFrame:\n",
    "    path = os.path.join(base_path, 'Stammdaten_Bayern.xlsx')\n",
    "    meta = pd.read_excel(path)\n",
    "    return meta\n",
    "\n",
    "# test it here\n",
    "metadata = read_meta(BASE)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the id column will be Stationsnummer\n",
    "id_column = 'Stationsnummer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file extract and parse\n",
    "\n",
    "I'll keep the files in the zip, just because. In baWü these zips are nicely flat-packed and there is actually no need to extract the zip. Later, we might want to extract and change the code below.\n",
    "\n",
    "bayern is really nasty as they change the format inside the files and they have negative water levels, which are most likely a sensor fault code or something. I build a dirty workaround for this by handling parser errors. If one occurs, the file content is written into a file-like-object in memory and splitted into a list of rows. Each row, that has a negative value on the second column is marked as faulty and skipped. If there were faulty columns, a warning containing the indices at which this error occured. The indices are all shifted by 8, as the first 8 rows contain metadata and are skipped anyway.\n",
    "Checked this procedure for one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to map ids to filenames\n",
    "def get_filename_mapping(zippath: str) -> Dict[str, str]:\n",
    "    with zipfile.ZipFile(zippath) as z:\n",
    "        m = dict()\n",
    "        for f in z.filelist:\n",
    "            id_only = os.path.basename(f.filename).split('.')[0]\n",
    "            m[str(id_only)] = f.filename\n",
    "        return m\n",
    "\n",
    "def get_file_from_zip(nr: Union[int, str], zippath: str, not_exists = 'raise'):\n",
    "    # get filename mapping\n",
    "    fmap = get_filename_mapping(zippath)\n",
    "    \n",
    "    # always use string\n",
    "    fname = str(nr)\n",
    "\n",
    "    # search the file \n",
    "    if fname in fmap.values():\n",
    "        fname = fname\n",
    "    elif fname in fmap.keys():\n",
    "        fname = fmap[fname]\n",
    "    else:\n",
    "        FileNotFoundError(f\"nr {nr} is nothing we would expect. Use a Stationsnummer or filename in the zip\")\n",
    "    \n",
    "    # go for the file\n",
    "    with zipfile.ZipFile(zippath) as z:\n",
    "        if fname not in [f.filename for f in z.filelist]:\n",
    "            # TODO: here, might want to warn and return an df filled with NAN\n",
    "            if not_exists == 'raise':\n",
    "                raise FileNotFoundError(f\"{fname} is not in {zippath}\")\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        # return the file content\n",
    "        return z.open(fname)\n",
    "        \n",
    "\n",
    "def extract_file(nr: Union[int, str], variable: str, zippath: str, not_exists = 'raise') -> pd.DataFrame:\n",
    "        # get the content\n",
    "        enc_content = get_file_from_zip(nr=nr, zippath=zippath, not_exists=not_exists)\n",
    "        if enc_content is None:\n",
    "            return pd.DataFrame(columns=['date', variable.lower(), 'flag'])\n",
    "        \n",
    "        # raw content\n",
    "        #raw = pd.read_csv(z.open(fname), encoding='latin1', skiprows=8, sep=' ', decimal=',', header=None)\n",
    "        try:\n",
    "            raw = pd.read_csv(enc_content, encoding='latin1', skiprows=8, sep=' ', decimal=',', header=None)\n",
    "        except ParserError:\n",
    "            enc_content.seek(0)\n",
    "            # In Bayern gibts negative Wasserstände ....\n",
    "            raw = enc_content.read().decode('latin1').splitlines()\n",
    "            faulty_rows = [float(row.split(' ')[1]) < 0 for row in raw[8:]]\n",
    "            raw = [row for row, faulty in zip(raw[8:], faulty_rows) if not faulty]\n",
    "            \n",
    "            # create in-memory buffer and read the CSV from memory\n",
    "            buffer = StringIO('\\n'.join(raw))\n",
    "            buffer.seek(0)\n",
    "\n",
    "            try:\n",
    "                raw = pd.read_csv(buffer, sep=' ', decimal=',', header=None)\n",
    "                warnings.warn(f\"{nr};FormatError;Faulty rows in {nr};[{', '.join([str(i + 8) for i, fault in enumerate(faulty_rows) if fault])}]\")\n",
    "            except Exception as e:\n",
    "                # TODO: in most cases there are suddenly more lines - maybe someone has an idea how to fix this\n",
    "                warnings.warn(f\"{nr};ParserError;Nr: {nr} failed alltogether;{str(e)}\")\n",
    "                raw = pd.DataFrame(columns=['x1', 'x2', 'x3', 'x4'])\n",
    "        finally:\n",
    "            enc_content.close()\n",
    "\n",
    "        # rename the headers\n",
    "        # Bayern has more surprises: sometimes they skip releaselevel.\n",
    "        # But we need to check if it was always releaselevel that was missin\n",
    "        if len(raw.columns) == 3:\n",
    "            warnings.warn(f\"{nr};FormatError;Nr: {nr} raw file has only 3 columns;Assuming that 'releaselevel' is missing. Please check.\")\n",
    "            raw.columns = ['timestamp', 'value', 'status']\n",
    "        else:\n",
    "            raw.columns = ['timestamp', 'value' ,'status', 'releaselevel']\n",
    "\n",
    "        # parse data\n",
    "        return pd.DataFrame({\n",
    "            'date': [dt.strptime(str(t)[:8], '%Y%m%d') for t in raw.timestamp],\n",
    "            variable.lower(): raw.value.values,\n",
    "\n",
    "            # TODO: Was bedeuten die flags hier?\n",
    "            'flag': [None for _ in raw.status],\n",
    "\n",
    "        })\n",
    "\n",
    "# test \n",
    "#m = get_filename_mapping(os.path.join(BASE, 'Abflüsse.zip'))\n",
    "#key = list(m.keys())[124]\n",
    "#print(key)\n",
    "key = 14106504\n",
    "#f = get_file_from_zip(key, os.path.join(BASE, 'Abflüsse.zip'))\n",
    "#df = pd.read_csv(f, encoding='latin1', skiprows=8, header=None, sep=' ', decimal=',')\n",
    "#print(f.read().decode('latin1').splitlines()[200:215])\n",
    "#f.close()\n",
    "\n",
    "df = extract_file(key, 'q', os.path.join(BASE, 'Abflüsse.zip'))\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is potentially interesting metadata in the header. Let's extract timezone and unit information and re-write the metadata extraction function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function \n",
    "def read_meta(base_path, scan_files: bool = True) -> pd.DataFrame:\n",
    "    # get the Stammdaten\n",
    "    path = os.path.join(base_path, 'Stammdaten_Bayern.xlsx')\n",
    "    meta = pd.read_excel(path)\n",
    "    \n",
    "    # now check for each file, if there is Stuff in the files\n",
    "    # list of q and w with tz, and unit array each\n",
    "    if not scan_files:\n",
    "        return meta\n",
    "    extras = [[[], []], [[], []]]\n",
    "    for nr in tqdm(metadata.Stationsnummer):\n",
    "        for i, _zip in enumerate(('Abflüsse.zip', 'Wasserstände.zip')):\n",
    "            f = get_file_from_zip(nr, os.path.join(base_path, 'Abflüsse.zip'), 'return_none')\n",
    "            if f is None:\n",
    "                tup = (None, None,)\n",
    "            else:\n",
    "                tup = f.read().decode('latin1').splitlines()[5:7]\n",
    "                f.close()\n",
    "            \n",
    "            # append\n",
    "            extras[i][0].append(tup[0])\n",
    "            extras[i][1].append(tup[1])\n",
    "\n",
    "    # now append the arrays to meta\n",
    "    meta['timezone_q'] = extras[0][0]\n",
    "    meta['unit_q'] = extras[0][1]\n",
    "    meta['timezone_w'] = extras[1][0]\n",
    "    meta['unit_w'] = extras[1][1]\n",
    "    \n",
    "    return meta\n",
    "\n",
    "# test it here\n",
    "metadata = read_meta(BASE)\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally run\n",
    "\n",
    "Now, the Q and W data can be extracted along with the metadata. The cool thing is, that all the id creation, data creation, merging and the mapping from our ids to the original ids and files is done by the context. This is helpful, as we less likely screw something up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Bundesland('Bayern') as bl:\n",
    "    # save the metadata\n",
    "    bl.save_raw_metadata(metadata, id_column, overwrite=True)\n",
    "\n",
    "    # for reference, call the nuts-mapping as table\n",
    "    nuts_map = bl.nuts_table\n",
    "    print(nuts_map.head)\n",
    "    \n",
    "    \n",
    "    # join the path for two zips\n",
    "    q_zip_path = os.path.join(bl.input_path, 'Abflüsse.zip')\n",
    "    w_zip_path = os.path.join(bl.input_path, 'Wasserstände.zip')\n",
    "    \n",
    "    with warnings.catch_warnings(record=True) as warns:\n",
    "        # go for all ids\n",
    "        for provider_id in tqdm(nuts_map.provider_id):\n",
    "            # extract the file for this provider\n",
    "            try:\n",
    "                q_df = extract_file(provider_id, 'q', q_zip_path, not_exists='fill_nan')\n",
    "                w_df = extract_file(provider_id, 'w', w_zip_path, not_exists='fill_nan')\n",
    "            except Exception:\n",
    "                print(provider_id)\n",
    "                break\n",
    "\n",
    "            # save\n",
    "            bl.save_timeseries(q_df, provider_id)\n",
    "            bl.save_timeseries(w_df, provider_id)\n",
    "\n",
    "        # check if there were warnings (there are warnings)\n",
    "        if len(warns) > 0:\n",
    "            log_path = bl.save_warnings(warns)\n",
    "            print(f\"There were warnings during the processing. The log can be found at: {log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c57ebfed52ffd848a0d2f36f1ea9c0a9060c9b67397fbb725d6aa92a9494b08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
