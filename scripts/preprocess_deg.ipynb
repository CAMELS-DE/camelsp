{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thüringen\n",
    "\n",
    "Every federal state is represented by its own input directory and is processed into a NUTS level 2 directory containing a sub-folder for each discharge location. These folder names are derived from NUTS and reflect the CAMELS id. The NUTS level 2 code for Thüringen is `DEG`.\n",
    "\n",
    "To pre-process the data, you need to write (at least) two functions. One should extract all metadata and condense it into a single `pandas.DataFrame`. This is used to build the folder structure and derive the ids.\n",
    "The second function has to take an id, as provided by the state authorities, called `provider_id` and return a `pandas.DataFrame` with the transformed data. The dataframe needs the three columns `['date', 'q' | 'w', 'flag']`.\n",
    "\n",
    "For easier and unified output handling, the `camelsp` package contains a context object called `Bundesland`. It takes a number of names and abbreviations to identify the correct federal state and returns an object that holds helper and save functions.\n",
    "\n",
    "The context saves files as needed and can easily be changed to save files with different strategies, ie. fill missing data with NaN, merge data into a single file, create files for each variable or pack everything together into a netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.errors import ParserError\n",
    "import os\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict, Tuple\n",
    "from datetime import datetime as dt\n",
    "from dateparser import parse\n",
    "import warnings\n",
    "from io import BytesIO\n",
    "\n",
    "from camelsp import Bundesland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context can also be instantiated as any regular Python class, ie. to load only the default input data path, that we will user later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the context also makes the input path available, if camelsp was install locally\n",
    "BASE = Bundesland('Thüringen').input_path\n",
    "BASE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse metadata\n",
    "\n",
    "Pegel metadata can be read quite easily. Only the separator is important as we have whitespaces in `'Gewässer'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Bundesland('Thüringen') as th:\n",
    "    metadata = pd.read_csv(os.path.join(th.input_path, 'Pegel_Metadaten.txt'), sep='\\t')\n",
    "metadata['unit_q'] = 'm³/s'\n",
    "metadata['unit_w'] = 'cm'\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The id_column is Pegelnr.\n",
    "id_column = 'Pegelnr.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Loading data is a bit more complicated. There is an extra header, but that is not important. Only the 'Einhait' contains important information, but that has been added manually to the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file(nr: Union[str, int], variable: str, base_path: str) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "    # always use str ids\n",
    "    nr = str(nr)\n",
    "    # merge the nr\n",
    "    if '.' in nr:\n",
    "        nr = nr.replace('.', '')\n",
    "\n",
    "    # build the filename\n",
    "    fname = f'{variable.lower()}{nr}.txt'\n",
    "\n",
    "    # build the path\n",
    "    path = os.path.join(base_path, variable.lower(), fname)\n",
    "\n",
    "    # return empty dataframe if data does not exist\n",
    "    if not os.path.exists(path):\n",
    "        return pd.DataFrame(columns=['date', variable.lower(), 'flag'])\n",
    "    \n",
    "    # otherwise read\n",
    "    df = pd.read_csv(path, skiprows=21, header=None, sep=\" \", parse_dates=[0], dayfirst=True, na_values=['Luecke', 'LUECKE'])\n",
    "    df.columns = ['date', 'hour', variable.lower(), 'comment']\n",
    "    \n",
    "    # check if there are comments\n",
    "    if not df.comment.isna().all():\n",
    "        print(df.comment)\n",
    "    \n",
    "    # build the flag column\n",
    "    df['flag'] = [np.isnan(c) for c in df.comment]\n",
    "    df.drop(['hour', 'comment'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "extract_file('42012.0', 'w', BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally run\n",
    "\n",
    "Now, the Q and W data can be extracted. The cool thing is, that all the id creation, data creation, merging and the mapping from our ids to the original ids and files is done by the context. This is helpful, as we less likely screw something up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Bundesland('Thüringen') as bl:\n",
    "    # save the metadata\n",
    "    bl.save_raw_metadata(metadata, id_column, overwrite=True)\n",
    "\n",
    "    # for reference, call the nuts-mapping as table\n",
    "    nuts_map = bl.nuts_table\n",
    "    print(nuts_map.head())\n",
    "\n",
    "    # go for each    \n",
    "    for _id in tqdm(metadata[id_column].values):\n",
    "        provider_id = str(_id)\n",
    "        # extract the two files\n",
    "        q_df = extract_file(provider_id, 'q', bl.input_path)\n",
    "        w_df = extract_file(provider_id, 'w', bl.input_path)\n",
    "\n",
    "        # save\n",
    "        bl.save_timeseries(q_df, provider_id)\n",
    "        bl.save_timeseries(w_df, provider_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54d8176e82297fa872ac8c77277e50c0e193f921954c1c4a0b1ae2e8be99b71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
