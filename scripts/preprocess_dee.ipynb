{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sachsen-Anhalt\n",
    "\n",
    "Every federal state is represented by its own input directory and is processed into a NUTS level 2 directory containing a sub-folder for each discharge location. These folder names are derived from NUTS and reflect the CAMELS id. The NUTS level 2 code for Sachsen-Anhalt is `DEE`.\n",
    "\n",
    "To pre-process the data, you need to write (at least) two functions. One should extract all metadata and condense it into a single `pandas.DataFrame`. This is used to build the folder structure and derive the ids.\n",
    "The second function has to take an id, as provided by the state authorities, called `provider_id` and return a `pandas.DataFrame` with the transformed data. The dataframe needs the three columns `['date', 'q' | 'w', 'flag']`.\n",
    "\n",
    "For easier and unified output handling, the `camelsp` package contains a context object called `Bundesland`. It takes a number of names and abbreviations to identify the correct federal state and returns an object that holds helper and save functions.\n",
    "\n",
    "The context saves files as needed and can easily be changed to save files with different strategies, ie. fill missing data with NaN, merge data into a single file, create files for each variable or pack everything together into a netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.errors import ParserError\n",
    "import os\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict, Tuple\n",
    "from datetime import datetime as dt\n",
    "from dateparser import parse\n",
    "import warnings\n",
    "from io import BytesIO\n",
    "\n",
    "from camelsp import Bundesland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context can also be instantiated as any regular Python class, ie. to load only the default input data path, that we will user later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the context also makes the input path available, if camelsp was install locally\n",
    "BASE = Bundesland('Sachsen-Anhalt').input_path\n",
    "BASE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data\n",
    "\n",
    "We do not have a Metadata file, but one Excel file for each station. Thus we need to parse each metadata individually and collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(BASE, 'LHW_*.DGJ'))\n",
    "print(f\"Found {len(files)} files.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Header on the first file, as we don't have any other Metadata file. We need a ZRXP parser. Couldn't find something simple quickly, so I write my own. https://prozessing.tbbm.at/zrxp/zrxp3.0_en.pdf this is the ZRXP specification. I will only implement the relevant metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names from the header\n",
    "# I will skip the energy market headers and remote logger headers\n",
    "HEADER = dict(\n",
    "    SANR='Alphanumerical station number',\n",
    "    SNAME='Station name',\n",
    "    SWATER='River name',\n",
    "    CMW='Values per day for equidistant time series values',\n",
    "    CNAME='Parameter name',\n",
    "    CNR='Parameter number',\n",
    "    CUNIT='Unit of the data value column',\n",
    "    RINVAL='Value for missing or invalid data record',\n",
    "    RTIMELVL='Time series time level',\n",
    "    TZ='time zone of all time stamps in the time series block, both header and data',\n",
    "    LAYOUT='specifies the column layout for the ZRXP data'\n",
    ")\n",
    "\n",
    "def extract_file(path: str) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "    # Get the header lines first\n",
    "    collection = []\n",
    "    headerlines = 0\n",
    "\n",
    "    # first read the head\n",
    "    with open(path, 'rb') as f:\n",
    "        # go for each line\n",
    "        for l in f.readlines():\n",
    "            if not l.decode('latin1').startswith('#'):\n",
    "                break\n",
    "            else:\n",
    "                # collect the header\n",
    "                collection.extend([_ for _ in l.decode('latin1').replace('#', '').split('|*|') if _ not in ('', '\\n', '\\r\\n')])\n",
    "                headerlines += 1\n",
    "\n",
    "    # create metadata container\n",
    "    meta = {}\n",
    "\n",
    "    # go for each collected header\n",
    "    for co in collection:\n",
    "        HEAD = [k for k in HEADER.keys() if co.startswith(k)]\n",
    "        if len(HEAD) == 1:\n",
    "            HEAD = HEAD[0]\n",
    "            meta[HEAD] = co.replace(HEAD, '')\n",
    "        elif len(HEAD) == 0:\n",
    "            if 'COMMENT' in meta:\n",
    "                meta['COMMENT'] += f\" {co}\"\n",
    "            else:\n",
    "                meta[\"COMMENT\"] = co\n",
    "        elif len(HEAD) > 1:\n",
    "            warnings.warn(f\"Can't parse header {co}\")\n",
    "\n",
    "    # now the data\n",
    "    header = meta['LAYOUT'].strip('()').split(',') if 'LAYOUT' in meta else [0, 1, 2, 3, 4, 5]\n",
    "    df = pd.read_csv(path, encoding='latin1', sep=' ', header=None, skiprows=headerlines, names=header, parse_dates=[0], na_values=int(meta.get('RINVAL', '-777')))\n",
    "\n",
    "    return meta, df\n",
    "\n",
    "extract_file(files[-1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go go for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# container\n",
    "meta = []\n",
    "raw_data = []\n",
    "\n",
    "with warnings.catch_warnings(record=True) as warn:\n",
    "    for fname in tqdm(files):\n",
    "        m, df = extract_file(fname)\n",
    "        meta.append(m)\n",
    "        raw_data.append(df)\n",
    "\n",
    "print(f\"Parsed {len(meta)} files with {len(warn)} warnings.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create metadata\n",
    "\n",
    "This should be pretty straightforward, but maybe not super-helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.DataFrame(meta)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'SANR'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "status = []\n",
    "int_type = []\n",
    "\n",
    "for m, df in zip(meta, raw_data):\n",
    "    # get status\n",
    "    for s in df.status.unique():\n",
    "        if s not in status:\n",
    "            status.append(s)\n",
    "    \n",
    "    # get interpolation types\n",
    "    for t in df.interpolation_type.unique():\n",
    "        if t not in int_type:\n",
    "            int_type.append(t)\n",
    "    \n",
    "    # make the df\n",
    "    out = df.iloc[:, :2].copy()\n",
    "    out.columns = ['date', 'q' if m['CNAME'].startswith('Q') else 'w']\n",
    "    out['flag'] = np.NaN\n",
    "    data.append(out)\n",
    "\n",
    "print(f\"Stauts:              {status}\")\n",
    "print(f\"Interpolation types: {int_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally run\n",
    "\n",
    "Now, the Q and W data can be extracted. The cool thing is, that all the id creation, data creation, merging and the mapping from our ids to the original ids and files is done by the context. This is helpful, as we less likely screw something up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Bundesland('Sachsen-Anhalt') as bl:\n",
    "    # save the metadata\n",
    "    bl.save_raw_metadata(metadata, id_column, overwrite=True)\n",
    "\n",
    "    # for reference, call the nuts-mapping as table\n",
    "    nuts_map = bl.nuts_table\n",
    "    print(nuts_map.head())\n",
    "\n",
    "    # go for each    \n",
    "    for m, df in tqdm(zip(meta, data), total=len(meta)):\n",
    "        \n",
    "        # get the provider id\n",
    "        provider_id = str(m[id_column])\n",
    "        bl.save_timeseries(df, provider_id)\n",
    "\n",
    "    # check if there were warnings (there are warnings)\n",
    "    if len(warn) > 0:\n",
    "        log_path = bl.save_warnings(warn)\n",
    "        print(f\"There were warnings during the processing. The log can be found at: {log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54d8176e82297fa872ac8c77277e50c0e193f921954c1c4a0b1ae2e8be99b71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
