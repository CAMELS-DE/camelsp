{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sachsen-Anhalt\n",
    "\n",
    "Every federal state is represented by its own input directory and is processed into a NUTS level 2 directory containing a sub-folder for each discharge location. These folder names are derived from NUTS and reflect the CAMELS id. The NUTS level 2 code for Sachsen-Anhalt is `DEE`.\n",
    "\n",
    "To pre-process the data, you need to write (at least) two functions. One should extract all metadata and condense it into a single `pandas.DataFrame`. This is used to build the folder structure and derive the ids.\n",
    "The second function has to take an id, as provided by the state authorities, called `provider_id` and return a `pandas.DataFrame` with the transformed data. The dataframe needs the three columns `['date', 'q' | 'w', 'flag']`.\n",
    "\n",
    "For easier and unified output handling, the `camelsp` package contains a context object called `Bundesland`. It takes a number of names and abbreviations to identify the correct federal state and returns an object that holds helper and save functions.\n",
    "\n",
    "The context saves files as needed and can easily be changed to save files with different strategies, ie. fill missing data with NaN, merge data into a single file, create files for each variable or pack everything together into a netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.errors import ParserError\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict, Tuple\n",
    "from datetime import datetime as dt\n",
    "from dateparser import parse\n",
    "import warnings\n",
    "from io import BytesIO\n",
    "\n",
    "from camelsp import Bundesland"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context can also be instantiated as any regular Python class, ie. to load only the default input data path, that we will user later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/alexander/Github/camels/camelsp/input_data/SA_Sachsen-Anhalt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the context also makes the input path available, if camelsp was install locally\n",
    "BASE = Bundesland('Sachsen-Anhalt').input_path\n",
    "BASE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data\n",
    "\n",
    "We do not have a Metadata file, but one Excel file for each station. Thus we need to parse each metadata individually and collect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patool: Extracting /home/alexander/Github/camels/camelsp/input_data/SA_Sachsen-Anhalt/TagMittel_DGJ_LSA.7z ...\n",
      "patool: running /usr/bin/7z x -o/home/alexander/Github/camels/camelsp/input_data/SA_Sachsen-Anhalt -- /home/alexander/Github/camels/camelsp/input_data/SA_Sachsen-Anhalt/TagMittel_DGJ_LSA.7z\n",
      "patool: ... /home/alexander/Github/camels/camelsp/input_data/SA_Sachsen-Anhalt/TagMittel_DGJ_LSA.7z extracted to `/home/alexander/Github/camels/camelsp/input_data/SA_Sachsen-Anhalt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/alexander/Github/camels/camelsp/input_data/SA_Sachsen-Anhalt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import patoolib\n",
    "\n",
    "# remove folder TagMittel_DGJ_20221007 to ensure that data is extracted freshly and is up to date\n",
    "if os.path.exists(f\"{BASE}/TagMittel_DGJ_20221007\"):\n",
    "    shutil.rmtree(f\"{BASE}/TagMittel_DGJ_20221007\")\n",
    "\n",
    "# extract .7z archive, make sure that 7z is installed to make patoolib work\n",
    "patoolib.extract_archive(f\"{BASE}/TagMittel_DGJ_LSA.7z\", outdir=BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 252 files.\n"
     ]
    }
   ],
   "source": [
    "files = glob(os.path.join(f\"{BASE}/TagMittel_DGJ_20221007\", 'LHW_*.DGJ'))\n",
    "print(f\"Found {len(files)} files.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Header on the first file, as we don't have any other Metadata file. We need a ZRXP parser. Couldn't find something simple quickly, so I write my own. https://prozessing.tbbm.at/zrxp/zrxp3.0_en.pdf this is the ZRXP specification. I will only implement the relevant metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'COMMENT': 'ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXPV2R2_E SOURCESYSTEMWISKI SOURCEIDa1a8b6cd-7ab3-4c4d-896b-a12975df58f0 TSPATH/LHW/578350/Q.DGJ/Day.Mean.DGJ',\n",
       "  'SANR': '578350',\n",
       "  'SNAME': 'Unterrißdorf',\n",
       "  'SWATER': 'Böse Sieben',\n",
       "  'CNR': 'Q.DGJ',\n",
       "  'CNAME': 'Q.DGJ',\n",
       "  'TZ': 'UTC+1',\n",
       "  'RINVAL': '-777',\n",
       "  'CUNIT': 'm³/s',\n",
       "  'LAYOUT': '(timestamp,value,status,interpolation_type,remark)'},\n",
       "        timestamp     value  status  interpolation_type remark\n",
       " 0     1978-11-01  0.196000      40                 603    NaN\n",
       " 1     1978-11-02  0.196000      40                 603    NaN\n",
       " 2     1978-11-03  0.196000      40                 603    NaN\n",
       " 3     1978-11-04  0.174000      40                 603    NaN\n",
       " 4     1978-11-05  0.174000      40                 603    NaN\n",
       " ...          ...       ...     ...                 ...    ...\n",
       " 15762 2021-12-27  0.023000    1064                 603    NaN\n",
       " 15763 2021-12-28  0.036802    1064                 603    NaN\n",
       " 15764 2021-12-29  0.045302    1064                 603    NaN\n",
       " 15765 2021-12-30  0.040031    1064                 603    NaN\n",
       " 15766 2021-12-31  0.031000    1064                 603    NaN\n",
       " \n",
       " [15767 rows x 5 columns])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the names from the header\n",
    "# I will skip the energy market headers and remote logger headers\n",
    "HEADER = dict(\n",
    "    SANR='Alphanumerical station number',\n",
    "    SNAME='Station name',\n",
    "    SWATER='River name',\n",
    "    CMW='Values per day for equidistant time series values',\n",
    "    CNAME='Parameter name',\n",
    "    CNR='Parameter number',\n",
    "    CUNIT='Unit of the data value column',\n",
    "    RINVAL='Value for missing or invalid data record',\n",
    "    RTIMELVL='Time series time level',\n",
    "    TZ='time zone of all time stamps in the time series block, both header and data',\n",
    "    LAYOUT='specifies the column layout for the ZRXP data'\n",
    ")\n",
    "\n",
    "def extract_file(path: str) -> Tuple[Dict[str, str], pd.DataFrame]:\n",
    "    # Get the header lines first\n",
    "    collection = []\n",
    "    headerlines = 0\n",
    "\n",
    "    # first read the head\n",
    "    with open(path, 'rb') as f:\n",
    "        # go for each line\n",
    "        for l in f.readlines():\n",
    "            if not l.decode('latin1').startswith('#'):\n",
    "                break\n",
    "            else:\n",
    "                # collect the header\n",
    "                collection.extend([_ for _ in l.decode('latin1').replace('#', '').split('|*|') if _ not in ('', '\\n', '\\r\\n')])\n",
    "                headerlines += 1\n",
    "\n",
    "    # create metadata container\n",
    "    meta = {}\n",
    "\n",
    "    # go for each collected header\n",
    "    for co in collection:\n",
    "        HEAD = [k for k in HEADER.keys() if co.startswith(k)]\n",
    "        if len(HEAD) == 1:\n",
    "            HEAD = HEAD[0]\n",
    "            meta[HEAD] = co.replace(HEAD, '')\n",
    "        elif len(HEAD) == 0:\n",
    "            if 'COMMENT' in meta:\n",
    "                meta['COMMENT'] += f\" {co}\"\n",
    "            else:\n",
    "                meta[\"COMMENT\"] = co\n",
    "        elif len(HEAD) > 1:\n",
    "            warnings.warn(f\"Can't parse header {co}\")\n",
    "\n",
    "    # now the data\n",
    "    header = meta['LAYOUT'].strip('()').split(',') if 'LAYOUT' in meta else [0, 1, 2, 3, 4, 5]\n",
    "    df = pd.read_csv(path, encoding='latin1', sep=' ', header=None, skiprows=headerlines, names=header, parse_dates=[0], na_values=int(meta.get('RINVAL', '-777')))\n",
    "\n",
    "    return meta, df\n",
    "\n",
    "extract_file(files[-1])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go go for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [05:14<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 252 files with 0 warnings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# container\n",
    "meta = []\n",
    "raw_data = []\n",
    "\n",
    "with warnings.catch_warnings(record=True) as warn:\n",
    "    for fname in tqdm(files):\n",
    "        m, df = extract_file(fname)\n",
    "        meta.append(m)\n",
    "        raw_data.append(df)\n",
    "\n",
    "print(f\"Parsed {len(meta)} files with {len(warn)} warnings.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create metadata\n",
    "\n",
    "This should be pretty straightforward, but maybe not super-helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT</th>\n",
       "      <th>SANR</th>\n",
       "      <th>SNAME</th>\n",
       "      <th>SWATER</th>\n",
       "      <th>CNR</th>\n",
       "      <th>CNAME</th>\n",
       "      <th>TZ</th>\n",
       "      <th>RINVAL</th>\n",
       "      <th>CUNIT</th>\n",
       "      <th>LAYOUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>578410</td>\n",
       "      <td>Wippra</td>\n",
       "      <td>Wipper</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>575310</td>\n",
       "      <td>Brücken</td>\n",
       "      <td>Kleine Helme</td>\n",
       "      <td>W.DGJ</td>\n",
       "      <td>W.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>cm</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>594005</td>\n",
       "      <td>Hagenau</td>\n",
       "      <td>Biese</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>444205</td>\n",
       "      <td>Ilsenburg</td>\n",
       "      <td>Ilse</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>579610</td>\n",
       "      <td>Meisdorf</td>\n",
       "      <td>Selke</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>587630</td>\n",
       "      <td>Parchen</td>\n",
       "      <td>Tucheim-Parchener Bach</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>555010</td>\n",
       "      <td>Dietrichsdorf</td>\n",
       "      <td>Zahna</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>576900</td>\n",
       "      <td>Oberthau</td>\n",
       "      <td>Weiße Elster</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>578220</td>\n",
       "      <td>Sennewitz</td>\n",
       "      <td>Götsche</td>\n",
       "      <td>W.DGJ</td>\n",
       "      <td>W.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>cm</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...</td>\n",
       "      <td>578350</td>\n",
       "      <td>Unterrißdorf</td>\n",
       "      <td>Böse Sieben</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>Q.DGJ</td>\n",
       "      <td>UTC+1</td>\n",
       "      <td>-777</td>\n",
       "      <td>m³/s</td>\n",
       "      <td>(timestamp,value,status,interpolation_type,rem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               COMMENT    SANR          SNAME  \\\n",
       "0    ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  578410         Wippra   \n",
       "1    ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  575310        Brücken   \n",
       "2    ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  594005        Hagenau   \n",
       "3    ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  444205      Ilsenburg   \n",
       "4    ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  579610       Meisdorf   \n",
       "..                                                 ...     ...            ...   \n",
       "247  ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  587630        Parchen   \n",
       "248  ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  555010  Dietrichsdorf   \n",
       "249  ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  576900       Oberthau   \n",
       "250  ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  578220      Sennewitz   \n",
       "251  ZRXPVERSION2300.100 ZRXPCREATORKiIOSystem.ZRXP...  578350   Unterrißdorf   \n",
       "\n",
       "                     SWATER    CNR  CNAME     TZ RINVAL CUNIT  \\\n",
       "0                    Wipper  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "1              Kleine Helme  W.DGJ  W.DGJ  UTC+1   -777    cm   \n",
       "2                     Biese  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "3                      Ilse  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "4                     Selke  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "..                      ...    ...    ...    ...    ...   ...   \n",
       "247  Tucheim-Parchener Bach  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "248                   Zahna  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "249            Weiße Elster  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "250                 Götsche  W.DGJ  W.DGJ  UTC+1   -777    cm   \n",
       "251             Böse Sieben  Q.DGJ  Q.DGJ  UTC+1   -777  m³/s   \n",
       "\n",
       "                                                LAYOUT  \n",
       "0    (timestamp,value,status,interpolation_type,rem...  \n",
       "1    (timestamp,value,status,interpolation_type,rem...  \n",
       "2    (timestamp,value,status,interpolation_type,rem...  \n",
       "3    (timestamp,value,status,interpolation_type,rem...  \n",
       "4    (timestamp,value,status,interpolation_type,rem...  \n",
       "..                                                 ...  \n",
       "247  (timestamp,value,status,interpolation_type,rem...  \n",
       "248  (timestamp,value,status,interpolation_type,rem...  \n",
       "249  (timestamp,value,status,interpolation_type,rem...  \n",
       "250  (timestamp,value,status,interpolation_type,rem...  \n",
       "251  (timestamp,value,status,interpolation_type,rem...  \n",
       "\n",
       "[252 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.DataFrame(meta)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = 'SANR'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stauts:              [40, -2147483393, 1064, 3112, 552, -2147482369, -2147482881, 200, -2147483608]\n",
      "Interpolation types: [603, 601, 604]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "status = []\n",
    "int_type = []\n",
    "\n",
    "for m, df in zip(meta, raw_data):\n",
    "    # get status\n",
    "    for s in df.status.unique():\n",
    "        if s not in status:\n",
    "            status.append(s)\n",
    "    \n",
    "    # get interpolation types\n",
    "    for t in df.interpolation_type.unique():\n",
    "        if t not in int_type:\n",
    "            int_type.append(t)\n",
    "    \n",
    "    # make the df\n",
    "    out = df.iloc[:, :2].copy()\n",
    "    out.columns = ['date', 'q' if m['CNAME'].startswith('Q') else 'w']\n",
    "    out['flag'] = np.NaN\n",
    "    data.append(out)\n",
    "\n",
    "print(f\"Stauts:              {status}\")\n",
    "print(f\"Interpolation types: {int_type}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally run\n",
    "\n",
    "Now, the Q and W data can be extracted. The cool thing is, that all the id creation, data creation, merging and the mapping from our ids to the original ids and files is done by the context. This is helpful, as we less likely screw something up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nuts_id provider_id                              path\n",
      "0  DEE10000      578410  ./DEE/DEE10000/DEE10000_data.csv\n",
      "1  DEE10010      575310  ./DEE/DEE10010/DEE10010_data.csv\n",
      "2  DEE10020      594005  ./DEE/DEE10020/DEE10020_data.csv\n",
      "3  DEE10030      444205  ./DEE/DEE10030/DEE10030_data.csv\n",
      "4  DEE10040      579610  ./DEE/DEE10040/DEE10040_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:45<00:00,  5.53it/s]\n"
     ]
    }
   ],
   "source": [
    "with Bundesland('Sachsen-Anhalt') as bl:\n",
    "    # save the metadata\n",
    "    bl.save_raw_metadata(metadata, id_column, overwrite=True)\n",
    "\n",
    "    # for reference, call the nuts-mapping as table\n",
    "    nuts_map = bl.nuts_table\n",
    "    print(nuts_map.head())\n",
    "\n",
    "    # go for each    \n",
    "    for m, df in tqdm(zip(meta, data), total=len(meta)):\n",
    "        \n",
    "        # get the provider id\n",
    "        provider_id = str(m[id_column])\n",
    "        bl.save_timeseries(df, provider_id)\n",
    "\n",
    "    # check if there were warnings (there are warnings)\n",
    "    if len(warn) > 0:\n",
    "        log_path = bl.save_warnings(warn)\n",
    "        print(f\"There were warnings during the processing. The log can be found at: {log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54d8176e82297fa872ac8c77277e50c0e193f921954c1c4a0b1ae2e8be99b71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
