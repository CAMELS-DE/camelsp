{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brandenburg\n",
    "\n",
    "Every federal state is represented by its own input directory and is processed into a NUTS level 2 directory containing a sub-folder for each discharge location. These folder names are derived from NUTS and reflect the CAMELS id. The NUTS level 2 code for Brandenburg is `DE4`.\n",
    "\n",
    "To pre-process the data, you need to write (at least) two functions. One should extract all metadata and condense it into a single `pandas.DataFrame`. This is used to build the folder structure and derive the ids.\n",
    "The second function has to take an id, as provided by the state authorities, called `provider_id` and return a `pandas.DataFrame` with the transformed data. The dataframe needs the three columns `['date', 'q' | 'w', 'flag']`.\n",
    "\n",
    "For easier and unified output handling, the `camelsp` package contains a context object called `Bundesland`. It takes a number of names and abbreviations to identify the correct federal state and returns an object that holds helper and save functions.\n",
    "\n",
    "The context saves files as needed and can easily be changed to save files with different strategies, ie. fill missing data with NaN, merge data into a single file, create files for each variable or pack everything together into a netcdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from typing import Union, Dict, Tuple\n",
    "import zipfile\n",
    "from datetime import datetime as dt\n",
    "from io import StringIO\n",
    "import warnings\n",
    "from dateparser import parse\n",
    "\n",
    "from camelsp import Bundesland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context can also be instantiated as any regular Python class, ie. to load only the default input data path, that we will user later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the context also makes the input path available, if camelsp was install locally\n",
    "BASE = Bundesland('Brandenburg').input_path\n",
    "BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata reader\n",
    "\n",
    "Define the function that extracts / reads and eventually merges all metadata for this federal state. You can develop the function here, without using the Bundesland context and then later use the context to pass extracted metadata. The Context has a function for saving *raw* metadata, that takes a `pandas.DataFrame` and needs you to identify the id column.\n",
    "Here, *raw* refers to provider metadata, that has not yet been transformed into the CAMELS-de Metadata schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, Brandenburg is quite cool. This will be hard to parse. Let's first extract the big Zip, because it's a zip of Excel, with MANY sheets. To make thing more complicated, they splitted the Excel files into two files, I guess because they got too large (haha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the ZIP in place\n",
    "if not os.path.exists(os.path.join(BASE, 'Q.TagWerte_1.xlsx')):\n",
    "    with zipfile.ZipFile(os.path.join(BASE, 'Anlage_4_W_Q-TagWerte.zip')) as z:\n",
    "        for f in z.filelist:\n",
    "            z.extract(f, BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the metadata from each sheet, for this, first read in all files and create a mapping from station ids to the sheet in the respective excel_file. \n",
    "\n",
    "Ah, nice, why call them by their name if you can just call them 'Zeitreihe_1', 'Zeitreihe_2' and so on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = pd.ExcelFile(os.path.join(BASE, 'Q.TagWerte_1.xlsx'))\n",
    "q2 = pd.ExcelFile(os.path.join(BASE, 'Q.TagWerte_2.xlsx'))\n",
    "w1 = pd.ExcelFile(os.path.join(BASE, 'W.TagWerte_1.xlsx'))\n",
    "w2 = pd.ExcelFile(os.path.join(BASE, 'W.TagWerte_2.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Zeitreihe_1'\n",
    "df = q1.parse(name, header=None, usecols=[0,1,2,3])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a tough sunday.\n",
    "\n",
    "The header is changing its size. I hope the blocks stay the same..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dirty_dataframe(df: pd.DataFrame, variable: str, skip_data: bool = False) -> Tuple[Dict, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parse the dirty dataframe directly read from the excel sheets.\n",
    "\n",
    "    \"\"\"\n",
    "    # get all empty row, as they separate the block, of course\n",
    "    null_idx = df[df.isnull().all(axis=1) == True].index.to_list()\n",
    "\n",
    "    # mark all block as None for now\n",
    "    ID = None\n",
    "    base = {}\n",
    "    loc = {}\n",
    "    co = {}\n",
    "    dat = None\n",
    "\n",
    "    # We expect three blocks here, base metadata, location metadata, coordinates and data\n",
    "    for i, (lo, up) in enumerate(zip([0] + null_idx, null_idx + [len(df)])):\n",
    "        # extract the block\n",
    "        block = df.iloc[lo:up, :]\n",
    "        \n",
    "        # switch the block\n",
    "        # data block - skipping for now\n",
    "        if i == 3 and not skip_data:\n",
    "            # on i==0, the ID should be filled, otherwise we have to handcraft this data\n",
    "            if ID is None:\n",
    "                continue\n",
    "            \n",
    "            # get the block\n",
    "            block = block.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
    "\n",
    "            # set the header\n",
    "            block.columns = block.iloc[0, :]\n",
    "            block.drop(block.index[0], axis=0, inplace=True)\n",
    "\n",
    "            # transform the data as everything is string, of course\n",
    "            dat = pd.DataFrame({\n",
    "                'date': [d.date() if isinstance(d, dt) else parse(d) for d in block.iloc[:, 0].values],\n",
    "                variable.lower(): block.iloc[:, 1].astype(float),\n",
    "                'flag': [fl.strip().lower() == 'geprÃ¼ft' for fl in block.iloc[:, 3].values]\n",
    "            })\n",
    "        \n",
    "        # base data or Standord\n",
    "        if i == 0 or i == 1:\n",
    "            block = block.iloc[:, :2].dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
    "            block = block.set_index(0).T.to_dict(orient='records')\n",
    "            if len(block) > 0:\n",
    "                if i == 0:\n",
    "                    if 'ID' in block[0].keys():\n",
    "                        ID = str(block[0]['ID'])\n",
    "                    else:\n",
    "                        warnings.warn(f\"Block #{i + 1}: No ID found. This will skip the data for this station.\")\n",
    "                    base = block[0]\n",
    "                else:\n",
    "                    loc = block[0]\n",
    "            else:\n",
    "                warnings.warn(f\"Block #{i + 1}: did not yield the correct shape. Please check the file. Skipping.\")\n",
    "        \n",
    "        # Koordinaten\n",
    "        elif i == 2:\n",
    "            # drop NaNs on both axis\n",
    "            block = block.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
    "            \n",
    "            # rename the CRS header\n",
    "            block.iloc[0, 0] = 'CRS'\n",
    "            block.columns = block.iloc[0]\n",
    "            block.drop(block.index[0], axis=0, inplace=True)\n",
    "            block = block.to_dict(orient='records')\n",
    "\n",
    "            if len(block) > 0:\n",
    "                co = block[0]\n",
    "            else:\n",
    "                warnings.warn(f\"Block #{i + 1} did not yield the correct shape. Please check the file. Skipping.\")\n",
    "\n",
    "    # now merge the metadata\n",
    "    meta = {**base, **loc, **co}\n",
    "\n",
    "    # if there was no metadata, set meta None again\n",
    "    if len(meta.keys()) == 0:\n",
    "        meta = None\n",
    "    \n",
    "    # finally return all we got\n",
    "    return meta, dat\n",
    "\n",
    "\n",
    "# Test the stuff\n",
    "parse_dirty_dataframe(df, 'q', skip_data=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate metadata and data in Memory\n",
    "\n",
    "I hope all the stuff can be put into Memory at once, otherwise we have to chunk it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty container for the data\n",
    "metadata, data, warns = [], [], []\n",
    "\n",
    "with warnings.catch_warnings(record=True) as wa:\n",
    "    # extract from each of the four excel sheets\n",
    "    for variable, xls in zip(('q', 'q', 'w', 'w'), (q1, q2, w1, w2)):\n",
    "        # go for each sheet\n",
    "        for sheet_name in tqdm(xls.sheet_names):\n",
    "            # load the dirty sheet\n",
    "            df = xls.parse(sheet_name, header=None, usecols=[0,1,2,3])\n",
    "\n",
    "            # parse it\n",
    "            meta, dat = parse_dirty_dataframe(df, variable)\n",
    "\n",
    "            if meta is not None:\n",
    "                metadata.append(meta),\n",
    "                data.append(dat)\n",
    "\n",
    "    # copy warnings\n",
    "    warns.extend(wa)\n",
    "\n",
    "print(f\"metadata length: {len(metadata)}    data files: {len(data)}      warnings:{len(warns)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There is more\n",
    "\n",
    "We have not only Anlage 4, but also Anlage 3. Check this file out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function \n",
    "def read_meta(base_path) -> pd.DataFrame:\n",
    "    path = os.path.join(base_path, 'Anlage_3.xlsx')\n",
    "    meta = pd.read_excel(path)\n",
    "    return meta\n",
    "\n",
    "# test it here\n",
    "other_meta = read_meta(BASE)\n",
    "pmeta = pd.DataFrame(metadata)\n",
    "\n",
    "# merge with the other, more interesting metadata\n",
    "#meta = pd.merge(pd.DataFrame(metadata), other_meta, left_on='ID', right_on='station_no', how='inner')\n",
    "\n",
    "print(f\"Overlapping names: {any([n in pmeta.Name.values for n in other_meta.station_name])}\")\n",
    "print(f\"Overlapping IDs: {any([str(i) in pmeta.Messstellennummer.values.tolist() for i in other_meta.station_no])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join both metadata together\n",
    "all_meta = pmeta.join(other_meta.set_index('station_no'), on='Messstellennummer', how='left')\n",
    "all_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the id column will be Messstellennummer\n",
    "id_column = 'Messstellennummer'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we have an ID everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All stations have an id: {all(['Messstellennummer' in m for m in metadata])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally run\n",
    "\n",
    "Save the stuff using the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Bundesland('Brandenburg') as bl:\n",
    "    # save the metadata\n",
    "    bl.save_raw_metadata(all_meta, id_column, overwrite=True)\n",
    "\n",
    "    # for reference, call the nuts-mapping as table\n",
    "    nuts_map = bl.nuts_table\n",
    "    print(nuts_map.head())\n",
    "    \n",
    "    # go for each of the files\n",
    "    for station_meta, station_data in tqdm(zip(metadata, data), total=len(metadata)):\n",
    "        # get the provider id\n",
    "        provider_id = station_meta['Messstellennummer']\n",
    "        \n",
    "        # save\n",
    "        bl.save_timeseries(station_data, provider_id)\n",
    "\n",
    "    # check if there were warnings (there are warnings)\n",
    "    if len(warns) > 0:\n",
    "        log_path = bl.save_warnings(warns)\n",
    "        print(f\"There were warnings during the processing. The log can be found at: {log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54d8176e82297fa872ac8c77277e50c0e193f921954c1c4a0b1ae2e8be99b71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
